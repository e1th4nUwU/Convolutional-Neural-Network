{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Implementaci√≥n de una red neuronal convolucional con NumPy para clasificaci√≥n de im√°genes : MNIST_ üñºÔ∏èüß†\n",
    "=========================================================================================================\n",
    "\n",
    "Introducci√≥n\n",
    "------------\n",
    "\n",
    "En este documento se detalla el proceso completo para construir, entrenar y evaluar una red neuronal convolucional (CNN) utilizando el conjunto de datos MNIST. Este conjunto de datos es ampliamente reconocido en el campo del aprendizaje autom√°tico y la visi√≥n por computadora, ya que consiste en 70,000 im√°genes de d√≠gitos escritos a mano del 0 al 9, cada una etiquetada con su correspondiente n√∫mero. üî¢\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "El objetivo principal es desarrollar un modelo de CNN que pueda aprender autom√°ticamente a reconocer y clasificar correctamente los d√≠gitos representados en las im√°genes de MNIST. Este proceso implica:\n",
    "\n",
    "*   **Preprocesamiento de Datos:** Las im√°genes se normalizan y se preparan para ser alimentadas al modelo. üìä\n",
    "    \n",
    "*   **Definici√≥n de la Arquitectura de la Red Neuronal:** Se establece la estructura de la red neuronal convolucional, incluyendo capas convolucionales, de pooling y completamente conectadas. üèóÔ∏è\n",
    "    \n",
    "*   **Entrenamiento del Modelo:** Se ajustan los pesos de la red utilizando el algoritmo de retropropagaci√≥n (backpropagation) con un m√©todo de optimizaci√≥n para minimizar una funci√≥n de p√©rdida. ‚öôÔ∏è\n",
    "    \n",
    "*   **Evaluaci√≥n del Rendimiento:** Se eval√∫a la precisi√≥n del modelo utilizando un conjunto de datos de prueba separado y se analizan los resultados obtenidos. ‚úÖ\n",
    "    \n",
    "\n",
    "Este proyecto no solo muestra c√≥mo implementar una red neuronal para reconocer d√≠gitos, sino que tambi√©n ofrece una visi√≥n general de los pasos necesarios para construir y entrenar modelos de aprendizaje autom√°tico en problemas de clasificaci√≥n de im√°genes. üìà\n",
    "\n",
    "A lo largo del documento, se explicar√°n detalladamente cada uno de estos pasos, junto con las decisiones de dise√±o y los resultados obtenidos durante el proceso de desarrollo del modelo de CNN para MNIST. üìö‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas Utilizadas\n",
    "\n",
    "### Instalaci√≥n de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (24.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in ./venv/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: np_utils in ./venv/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from keras) (13.8.1)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install numpy tensorflow keras np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy\n",
    "\n",
    "![NumpyLogo](img/numpy_logo.png)\n",
    "\n",
    "NumPy es una biblioteca fundamental para la computaci√≥n cient√≠fica en Python. Proporciona soporte para arreglos multidimensionales, matrices y una amplia variedad de funciones matem√°ticas de alto nivel para operar en estos arreglos. Es fundamental en el procesamiento num√©rico y el manejo eficiente de datos para aplicaciones de aprendizaje autom√°tico.\n",
    "\n",
    "#### Importaci√≥n:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "![TensorFlowLogo](img/tensorflow_logo.png)\n",
    "\n",
    "TensorFlow es una biblioteca de c√≥digo abierto desarrollada por Google para realizar c√°lculos num√©ricos y construir modelos de aprendizaje autom√°tico. Es una de las bibliotecas m√°s populares para el desarrollo de modelos de aprendizaje profundo y redes neuronales.\n",
    "\n",
    "## Keras\n",
    "\n",
    "![KerasLogo](img/keras_logo.png)\n",
    "\n",
    "Keras es una biblioteca de redes neuronales de c√≥digo abierto escrita en Python. Es capaz de ejecutarse sobre TensorFlow, Microsoft Cognitive Toolkit o Theano. Fue desarrollada con la idea de facilitar la experimentaci√≥n en el campo del aprendizaje profundo. Para esta ocasi√≥n, utilizaremos Keras con TensorFlow como backend y tambi√©n utilizaremos uno de los conjuntos de datos que vienen incluidos en Keras.\n",
    "\n",
    "## Conjunto de Datos MNIST\n",
    "\n",
    "El conjunto de datos MNIST es un conjunto est√°ndar de datos de d√≠gitos escritos a mano ampliamente utilizado para entrenar y probar modelos de aprendizaje autom√°tico en el campo del reconocimiento √≥ptico de caracteres (OCR). Consiste en un conjunto de 70,000 im√°genes en escala de grises de d√≠gitos escritos a mano, cada una de tama√±o 28x28 p√≠xeles. Estas im√°genes est√°n etiquetadas con el d√≠gito correspondiente del 0 al 9.\n",
    "\n",
    "### Caracter√≠sticas del Conjunto de Datos:\n",
    "\n",
    "- **Im√°genes:** Cada imagen representa un d√≠gito del 0 al 9.\n",
    "- **Tama√±o:** Cada imagen tiene dimensiones de 28x28 p√≠xeles.\n",
    "- **Etiquetas:** Cada imagen est√° etiquetada con el d√≠gito que representa.\n",
    "\n",
    "El objetivo t√≠pico al trabajar con MNIST es entrenar un modelo de aprendizaje autom√°tico para reconocer y clasificar correctamente los d√≠gitos escritos a mano bas√°ndose √∫nicamente en las im√°genes de entrada.\n",
    "\n",
    "![MNIST Dataset](img/MnistExamplesModified.png)\n",
    "\n",
    "## Importaci√≥n del dataset MNIST y las herramientas necesarias para trabajar con √©l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 07:00:17.540314: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-25 07:00:17.540790: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-25 07:00:17.544715: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-25 07:00:17.555318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 07:00:17.576333: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 07:00:17.582624: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 07:00:17.601890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 07:00:18.692875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.utils as np_utils\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas Layer and Dense\n",
    "### Clase `Layer`\n",
    "La clase Layer sirve como una clase base para todas las capas de una red neuronal. Contiene dos m√©todos esenciales, forward y backward, que deben ser implementados en las clases derivadas. Estos m√©todos representan el pase hacia adelante y el pase hacia atr√°s de la red.\n",
    "\n",
    "Pase hacia adelante (Forward Pass):\n",
    "Recibe una entrada de la capa anterior y calcula la salida para ser enviada a la siguiente capa.\n",
    "Pase hacia atr√°s (Backward Pass):\n",
    "Recibe el gradiente de la salida (de la capa siguiente) y actualiza los pesos u otros par√°metros en base a la tasa de aprendizaje. Tambi√©n calcula el gradiente para la entrada, que se propagar√° hacia atr√°s.\n",
    "Atributos\n",
    "input: Los datos de entrada para la capa, almacenados durante el pase hacia adelante.\n",
    "output: La salida de la capa despu√©s del pase hacia adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n\n",
    "\n",
    "La clase `Layer` es una clase base que define los m√©todos esenciales de una capa en una red neuronal. Incluye los m√©todos `forward` y `backward`, que son fundamentales para el entrenamiento de la red. Estos m√©todos deben ser implementados en las subclases derivadas de `Layer`.\n",
    "\n",
    "### M√©todos\n",
    "\n",
    "*   **`forward(input)`** : Este m√©todo define el pase hacia adelante en la red. Toma como entrada `input` y devuelve la salida correspondiente de la capa. En la implementaci√≥n base no realiza ninguna operaci√≥n, ya que se espera que las subclases lo definan.\n",
    "    \n",
    "*   **`backward(output_gradient, learning_rate)`** : Este m√©todo define la retropropagaci√≥n de la red. Recibe como par√°metros el gradiente de la salida (`output_gradient`) y la tasa de aprendizaje (`learning_rate`). Actualiza los par√°metros de la capa en base a estos valores. Similar al pase hacia adelante, su funcionalidad debe ser implementada en las subclases.\n",
    "    \n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Dense` (Capa Totalmente Conectada)\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input\n",
    "):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n\n",
    "\n",
    "La clase `Dense` implementa una capa totalmente conectada, donde cada neurona de la capa est√° conectada a todas las neuronas de la capa anterior. Se utiliza tanto para aprender caracter√≠sticas complejas como para realizar predicciones.\n",
    "\n",
    "### Atributos\n",
    "\n",
    "*   **`weights`** : Matriz de pesos con dimensiones `(output_size, input_size)` inicializada aleatoriamente. Estos pesos determinan la influencia de cada neurona de la capa anterior sobre las neuronas de la capa actual.\n",
    "    \n",
    "*   **`bias`** : Vector de sesgos con dimensiones `(output_size, 1)` tambi√©n inicializado aleatoriamente. El sesgo se suma a la salida de la multiplicaci√≥n de los pesos y la entrada.\n",
    "    \n",
    "\n",
    "### M√©todos\n",
    "\n",
    "*   **`forward(input)`**: Realiza el pase hacia adelante multiplicando la entrada por los pesos y sumando el sesgo. La ecuaci√≥n que describe esta operaci√≥n es:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{salida} = W \\cdot X + b\n",
    "\\end{aligned}    \n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "*   $W$ es la matriz de pesos.\n",
    "*   $X$ es el vector de entrada.\n",
    "*   $b$ es el vector de sesgos.\n",
    "\n",
    "*   **`backward(output_gradient, learning_rate)`**: Realiza la retropropagaci√≥n, calculando el gradiente de los pesos y el sesgo a partir del gradiente de la salida. Luego, actualiza los pesos y el sesgo con la tasa de aprendizaje.\n",
    "\n",
    "    Los gradientes se calculan de la siguiente manera:\n",
    "    \n",
    "    *   **Gradiente de los pesos**:\n",
    "\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial W} = \\text{gradiente\\_salida} \\cdot X^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    *   **Gradiente de la entrada**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial X} = W^T \\cdot \\text{gradiente\\_salida}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finalmente, los pesos y sesgos se actualizan con:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    W = W - \\text{tasa\\_aprendizaje} \\cdot \\frac{\\partial L}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    b = b - \\text{tasa\\_aprendizaje} \\cdot \\frac{\\partial L}{\\partial b}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase `Activation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n\n",
    "\n",
    "La clase `Activation` representa una capa de activaci√≥n en una red neuronal. Utiliza una funci√≥n de activaci√≥n dada y su derivada para aplicar transformaciones no lineales a la entrada durante el pase hacia adelante y hacia atr√°s.\n",
    "\n",
    "### M√©todos\n",
    "\n",
    "*   **`forward(input)`** : Realiza el pase hacia adelante aplicando la funci√≥n de activaci√≥n a la entrada y guarda la entrada para su uso posterior en la retropropagaci√≥n.\n",
    "    \n",
    "*   **`backward(output_gradient, learning_rate)`** : Realiza el pase hacia atr√°s multiplicando el gradiente de salida por la derivada de la funci√≥n de activaci√≥n evaluada en la entrada guardada. Este m√©todo ajusta la retropropagaci√≥n de acuerdo con la transformaci√≥n no lineal aplicada en el pase hacia adelante.\n",
    "    \n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Tanh`\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n\n",
    "\n",
    "La clase `Tanh` implementa la funci√≥n de activaci√≥n tangente hiperb√≥lica y su derivada. Hereda de `Activation`, especificando la funci√≥n `tanh` y su derivada `tanh_prime` como los m√©todos de activaci√≥n y su derivada respectivamente.\n",
    "\n",
    "### M√©todos\n",
    "\n",
    "No se agregan m√©todos adicionales m√°s all√° de los heredados de `Activation`.\n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Sigmoid`\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "\n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n\n",
    "\n",
    "La clase `Sigmoid` implementa la funci√≥n de activaci√≥n sigmoide y su derivada. Al igual que `Tanh`, hereda de `Activation`, especificando la funci√≥n `sigmoid` y su derivada `sigmoid_prime` como los m√©todos de activaci√≥n y su derivada respectivamente.\n",
    "\n",
    "### M√©todos\n",
    "\n",
    "No se agregan m√©todos adicionales m√°s all√° de los heredados de `Activation`.\n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Softmax`\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input\n",
    "):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripci√≥n\n",
    "\n",
    "La clase `Softmax` implementa la funci√≥n de activaci√≥n softmax, com√∫nmente utilizada en la capa de salida de una red neuronal para problemas de clasificaci√≥n multiclase. Calcula las probabilidades normalizadas de clases diferentes y sus gradientes durante el pase hacia adelante y hacia atr√°s, respectivamente.\n",
    "\n",
    "### M√©todos\n",
    "\n",
    "*   **`forward(input)`** : Realiza el pase hacia adelante aplicando la funci√≥n softmax a la entrada. Calcula exponenciales de la entrada, normaliza para obtener probabilidades y guarda el resultado en `self.output`.\n",
    "    \n",
    "*   **`backward(output_gradient, learning_rate)`** : Realiza el pase hacia atr√°s aplicando la derivada de softmax a `output_gradient`. Utiliza una forma optimizada de calcular el gradiente en comparaci√≥n con la versi√≥n original, mejorando la eficiencia computacional durante la retropropagaci√≥n.\n",
    "\n",
    "\n",
    "## Funciones de P√©rdida y Derivadas\n",
    "\n",
    "### Funci√≥n de Error Cuadr√°tico Medio (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el error cuadr√°tico medio entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - float: Error cuadr√°tico medio.\n",
    "    \"\"\"\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula la derivada del error cuadr√°tico medio respecto a las predicciones.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Gradiente del error cuadr√°tico medio.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥n de Entrop√≠a Cruzada Binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula la entrop√≠a cruzada binaria entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - float: Entrop√≠a cruzada binaria.\n",
    "    \"\"\"\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de la entrop√≠a cruzada binaria respecto a las predicciones.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Gradiente de la entrop√≠a cruzada binaria.\n",
    "    \"\"\"\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Predicci√≥n y Entrenamiento de Redes Neuronales\n",
    "### Funci√≥n de Predicci√≥n (`predict`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    \"\"\"\n",
    "    Realiza una predicci√≥n utilizando una red neuronal dada.\n",
    "\n",
    "    Args:\n",
    "    - network (list): Lista de capas de la red neuronal.\n",
    "    - input (numpy array): Entrada para la predicci√≥n.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Salida de la red neuronal despu√©s de aplicar todas las capas.\n",
    "    \"\"\"\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥n de Entrenamiento (`train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, loss, loss_prime, x_train, y_train, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    Entrena una red neuronal utilizando el algoritmo de retropropagaci√≥n.\n",
    "\n",
    "    Args:\n",
    "    - network (list): Lista de capas de la red neuronal.\n",
    "    - loss (function): Funci√≥n de p√©rdida para evaluar el error.\n",
    "    - loss_prime (function): Derivada de la funci√≥n de p√©rdida para retropropagar el error.\n",
    "    - x_train (numpy array): Datos de entrada de entrenamiento.\n",
    "    - y_train (numpy array): Valores verdaderos correspondientes a los datos de entrada.\n",
    "    - epochs (int): N√∫mero de √©pocas o iteraciones de entrenamiento (default: 1000).\n",
    "    - learning_rate (float): Tasa de aprendizaje para actualizar los pesos durante el entrenamiento (default: 0.01).\n",
    "    - verbose (bool): Flag para imprimir el progreso del entrenamiento (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = predict(network, x)\n",
    "\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funci√≥n de Preprocesamiento de Datos (`preprocess_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos de MNIST, transformando las im√°genes y las etiquetas para su uso en la red neuronal.\n",
    "\n",
    "    Args:\n",
    "    - x (numpy array): Datos de entrada (im√°genes).\n",
    "    - y (numpy array): Etiquetas correspondientes (n√∫meros del 0 al 9).\n",
    "    - limit (int): L√≠mite para el n√∫mero de datos a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Datos de entrada preprocesados.\n",
    "    - numpy array: Etiquetas preprocesadas y codificadas.\n",
    "    \"\"\"\n",
    "    # Reorganiza y normaliza los datos de entrada\n",
    "    x = x.reshape(x.shape[0], 28 * 28, 1)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    # Codifica la salida (n√∫meros del 0 al 9) en un vector de tama√±o 10\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(y.shape[0], 10, 1)\n",
    "    return x[:limit], y[:limit]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y Preprocesamiento de Datos MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definici√≥n de la Arquitectura de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Dense(28 * 28, 40),\n",
    "    Tanh(),\n",
    "    Dense(40, 10),\n",
    "    Tanh()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la Red Neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error=0.8727098636309831\n",
      "2/100, error=0.8005264391101434\n",
      "3/100, error=0.7560818280770303\n",
      "4/100, error=0.6925861391497522\n",
      "5/100, error=0.5908179474134135\n",
      "6/100, error=0.43961604290773637\n",
      "7/100, error=0.26194002219622825\n",
      "8/100, error=0.1738515989570927\n",
      "9/100, error=0.14476697212888012\n",
      "10/100, error=0.1332438957178444\n",
      "11/100, error=0.12730190206612657\n",
      "12/100, error=0.12260658720032985\n",
      "13/100, error=0.11869035231345593\n",
      "14/100, error=0.11579003262922616\n",
      "15/100, error=0.11369414201779399\n",
      "16/100, error=0.1116697405507871\n",
      "17/100, error=0.10985325118990776\n",
      "18/100, error=0.10831724712648368\n",
      "19/100, error=0.10678412216485142\n",
      "20/100, error=0.10530855873483609\n",
      "21/100, error=0.10324705450081957\n",
      "22/100, error=0.10150586079577471\n",
      "23/100, error=0.09972885028166886\n",
      "24/100, error=0.09815220083130037\n",
      "25/100, error=0.09732193067348896\n",
      "26/100, error=0.09599288066333622\n",
      "27/100, error=0.09469321959716677\n",
      "28/100, error=0.09371754537997211\n",
      "29/100, error=0.09248000910317403\n",
      "30/100, error=0.09151283978733823\n",
      "31/100, error=0.09061713627686903\n",
      "32/100, error=0.08977133961612252\n",
      "33/100, error=0.08887872485160775\n",
      "34/100, error=0.08808315551447601\n",
      "35/100, error=0.08737590228643695\n",
      "36/100, error=0.0867703107040453\n",
      "37/100, error=0.08621801464950492\n",
      "38/100, error=0.08552185846674243\n",
      "39/100, error=0.08480905465283742\n",
      "40/100, error=0.0840643254936738\n",
      "41/100, error=0.0836957801459345\n",
      "42/100, error=0.08296125133524188\n",
      "43/100, error=0.08250204997339178\n",
      "44/100, error=0.08196749478602071\n",
      "45/100, error=0.08130255157890724\n",
      "46/100, error=0.08087233655240691\n",
      "47/100, error=0.08027273938971762\n",
      "48/100, error=0.07951030897200749\n",
      "49/100, error=0.07895267673572902\n",
      "50/100, error=0.07849533908100602\n",
      "51/100, error=0.07782248170364905\n",
      "52/100, error=0.07731430359139538\n",
      "53/100, error=0.07687977122588233\n",
      "54/100, error=0.07629637580687904\n",
      "55/100, error=0.07603039928937748\n",
      "56/100, error=0.07550779769094881\n",
      "57/100, error=0.07502359813051183\n",
      "58/100, error=0.0746086610140651\n",
      "59/100, error=0.0742512178649907\n",
      "60/100, error=0.07369672925976506\n",
      "61/100, error=0.07334658705718314\n",
      "62/100, error=0.07309618774742933\n",
      "63/100, error=0.07264913371506065\n",
      "64/100, error=0.07231292578980322\n",
      "65/100, error=0.072141063988483\n",
      "66/100, error=0.07167180380827479\n",
      "67/100, error=0.07125737945818496\n",
      "68/100, error=0.07110336869291416\n",
      "69/100, error=0.07051853980133753\n",
      "70/100, error=0.07015322578370915\n",
      "71/100, error=0.06987203906693279\n",
      "72/100, error=0.06952700176491002\n",
      "73/100, error=0.0692894637564653\n",
      "74/100, error=0.06869648325062735\n",
      "75/100, error=0.06826940073888742\n",
      "76/100, error=0.06770466994744617\n",
      "77/100, error=0.06741746736119415\n",
      "78/100, error=0.06702173766201777\n",
      "79/100, error=0.06673240922326762\n",
      "80/100, error=0.06638120203240204\n",
      "81/100, error=0.0661659867337441\n",
      "82/100, error=0.06585795033956811\n",
      "83/100, error=0.06562210165025661\n",
      "84/100, error=0.06532063160993853\n",
      "85/100, error=0.06503629295591996\n",
      "86/100, error=0.06477424187267533\n",
      "87/100, error=0.06455198452047989\n",
      "88/100, error=0.06429798082690026\n",
      "89/100, error=0.06402742337296816\n",
      "90/100, error=0.06373884947549117\n",
      "91/100, error=0.0634880274088557\n",
      "92/100, error=0.06318673361116552\n",
      "93/100, error=0.06289059338488802\n",
      "94/100, error=0.06255712081737637\n",
      "95/100, error=0.062378074177676684\n",
      "96/100, error=0.061921576033813604\n",
      "97/100, error=0.06168182434955905\n",
      "98/100, error=0.06134467327997239\n",
      "99/100, error=0.06111810674976954\n",
      "100/100, error=0.06085693808094799\n"
     ]
    }
   ],
   "source": [
    "train(network, mse, mse_prime, x_train, y_train, epochs=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7 \ttrue: 7\n",
      "pred: 5 \ttrue: 2\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 0 \ttrue: 4\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 4 \ttrue: 4\n",
      "pred: 6 \ttrue: 9\n",
      "pred: 0 \ttrue: 5\n",
      "pred: 6 \ttrue: 9\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 0 \ttrue: 6\n",
      "pred: 6 \ttrue: 9\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 6 \ttrue: 1\n",
      "pred: 7 \ttrue: 5\n",
      "pred: 2 \ttrue: 9\n",
      "pred: 7 \ttrue: 7\n",
      "pred: 6 \ttrue: 3\n",
      "pred: 4 \ttrue: 4\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    print('pred:', np.argmax(output), '\\ttrue:', np.argmax(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En este proyecto, hemos explorado el desarrollo y entrenamiento de una red neuronal convolucional (CNN) para la clasificaci√≥n de d√≠gitos utilizando el conjunto de datos MNIST. A continuaci√≥n, se presentan las principales conclusiones y hallazgos obtenidos:\n",
    "\n",
    "### Logros\n",
    "\n",
    "- **Implementaci√≥n Exitosa de la CNN:** Se logr√≥ implementar y entrenar una CNN utilizando la biblioteca Keras sobre TensorFlow. La red neuronal pudo aprender a reconocer los d√≠gitos escritos a mano con una precisi√≥n significativa.\n",
    "\n",
    "- **Preprocesamiento Eficaz de Datos:** El preprocesamiento de las im√°genes de MNIST, incluyendo la normalizaci√≥n y la codificaci√≥n de las etiquetas, fue crucial para el √©xito del modelo.\n",
    "\n",
    "- **Aprendizaje Autom√°tico de Representaciones:** La red neuronal pudo aprender representaciones significativas de las im√°genes de d√≠gitos, lo que permiti√≥ una clasificaci√≥n precisa.\n",
    "\n",
    "### Desaf√≠os y Lecciones Aprendidas\n",
    "\n",
    "- **Ajuste de Hiperpar√°metros:** La selecci√≥n adecuada de hiperpar√°metros como la tasa de aprendizaje y el n√∫mero de √©pocas de entrenamiento fue crucial y requiri√≥ experimentaci√≥n y ajustes iterativos.\n",
    "\n",
    "- **Interpretaci√≥n de Resultados:** La evaluaci√≥n del modelo y la interpretaci√≥n de las m√©tricas de rendimiento fueron fundamentales para comprender su eficacia y posibles √°reas de mejora.\n",
    "\n",
    "### Futuras Direcciones\n",
    "\n",
    "- **Mejoras en el Modelo:** Se podr√≠an explorar arquitecturas m√°s complejas de CNN, as√≠ como t√©cnicas avanzadas como la regularizaci√≥n y el aumento de datos para mejorar a√∫n m√°s el rendimiento del modelo.\n",
    "\n",
    "- **Aplicaciones Pr√°cticas:** Este proyecto puede extenderse para aplicaciones pr√°cticas como sistemas de reconocimiento de caracteres en documentos escaneados o aplicaciones de OCR en tiempo real.\n",
    "\n",
    "En resumen, este proyecto no solo demuestra la aplicaci√≥n efectiva de t√©cnicas de aprendizaje autom√°tico para la clasificaci√≥n de im√°genes, sino que tambi√©n destaca la importancia de la experimentaci√≥n rigurosa y la evaluaci√≥n exhaustiva en el desarrollo de modelos de aprendizaje autom√°tico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
