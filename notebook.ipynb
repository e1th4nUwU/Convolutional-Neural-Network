{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación de una red neuronal convolucional con NumPy para clasificación de imágenes : MNIST\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En este documento se detalla el proceso completo para construir, entrenar y evaluar una red neuronal convolucional (CNN) utilizando el conjunto de datos MNIST. Este conjunto de datos es ampliamente reconocido en el campo del aprendizaje automático y la visión por computadora, ya que consiste en 70,000 imágenes de dígitos escritos a mano del 0 al 9, cada una etiquetada con su correspondiente número.\n",
    "\n",
    "### Objetivo\n",
    "\n",
    "El objetivo principal es desarrollar un modelo de CNN que pueda aprender automáticamente a reconocer y clasificar correctamente los dígitos representados en las imágenes de MNIST. Este proceso implica:\n",
    "\n",
    "- **Preprocesamiento de Datos:** Las imágenes se normalizan y se preparan para ser alimentadas al modelo.\n",
    "  \n",
    "- **Definición de la Arquitectura de la Red Neuronal:** Se establece la estructura de la red neuronal convolucional, incluyendo capas convolucionales, de pooling y completamente conectadas.\n",
    "\n",
    "- **Entrenamiento del Modelo:** Se ajustan los pesos de la red utilizando el algoritmo de retropropagación (backpropagation) con un método de optimización para minimizar una función de pérdida.\n",
    "\n",
    "- **Evaluación del Rendimiento:** Se evalúa la precisión del modelo utilizando un conjunto de datos de prueba separado y se analizan los resultados obtenidos.\n",
    "\n",
    "Este proyecto no solo muestra cómo implementar una red neuronal para reconocer dígitos, sino que también ofrece una visión general de los pasos necesarios para construir y entrenar modelos de aprendizaje automático en problemas de clasificación de imágenes.\n",
    "\n",
    "A lo largo del documento, se explicarán detalladamente cada uno de estos pasos, junto con las decisiones de diseño y los resultados obtenidos durante el proceso de desarrollo del modelo de CNN para MNIST.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bibliotecas Utilizadas\n",
    "\n",
    "### Instalación de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./venv/lib/python3.12/site-packages (24.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in ./venv/lib/python3.12/site-packages (1.26.4)\n",
      "Requirement already satisfied: tensorflow in ./venv/lib/python3.12/site-packages (2.17.0)\n",
      "Requirement already satisfied: keras in ./venv/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: np_utils in ./venv/lib/python3.12/site-packages (0.6.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in ./venv/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in ./venv/lib/python3.12/site-packages (from tensorflow) (0.4.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./venv/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in ./venv/lib/python3.12/site-packages (from tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.25.5)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in ./venv/lib/python3.12/site-packages (from tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./venv/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./venv/lib/python3.12/site-packages (from tensorflow) (1.66.1)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in ./venv/lib/python3.12/site-packages (from tensorflow) (2.17.1)\n",
      "Requirement already satisfied: rich in ./venv/lib/python3.12/site-packages (from keras) (13.8.1)\n",
      "Requirement already satisfied: namex in ./venv/lib/python3.12/site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in ./venv/lib/python3.12/site-packages (from keras) (0.12.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./venv/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./venv/lib/python3.12/site-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in ./venv/lib/python3.12/site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./venv/lib/python3.12/site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in ./venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./venv/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install --upgrade pip\n",
    "!{sys.executable} -m pip install numpy tensorflow keras np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy\n",
    "\n",
    "![NumpyLogo](img/numpy_logo.png)\n",
    "\n",
    "NumPy es una biblioteca fundamental para la computación científica en Python. Proporciona soporte para arreglos multidimensionales, matrices y una amplia variedad de funciones matemáticas de alto nivel para operar en estos arreglos. Es fundamental en el procesamiento numérico y el manejo eficiente de datos para aplicaciones de aprendizaje automático.\n",
    "\n",
    "#### Importación:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow\n",
    "\n",
    "![TensorFlowLogo](img/tensorflow_logo.png)\n",
    "\n",
    "TensorFlow es una biblioteca de código abierto desarrollada por Google para realizar cálculos numéricos y construir modelos de aprendizaje automático. Es una de las bibliotecas más populares para el desarrollo de modelos de aprendizaje profundo y redes neuronales.\n",
    "\n",
    "## Keras\n",
    "\n",
    "![KerasLogo](img/keras_logo.png)\n",
    "\n",
    "Keras es una biblioteca de redes neuronales de código abierto escrita en Python. Es capaz de ejecutarse sobre TensorFlow, Microsoft Cognitive Toolkit o Theano. Fue desarrollada con la idea de facilitar la experimentación en el campo del aprendizaje profundo. Para esta ocasión, utilizaremos Keras con TensorFlow como backend y también utilizaremos uno de los conjuntos de datos que vienen incluidos en Keras.\n",
    "\n",
    "## Conjunto de Datos MNIST\n",
    "\n",
    "El conjunto de datos MNIST es un conjunto estándar de datos de dígitos escritos a mano ampliamente utilizado para entrenar y probar modelos de aprendizaje automático en el campo del reconocimiento óptico de caracteres (OCR). Consiste en un conjunto de 70,000 imágenes en escala de grises de dígitos escritos a mano, cada una de tamaño 28x28 píxeles. Estas imágenes están etiquetadas con el dígito correspondiente del 0 al 9.\n",
    "\n",
    "### Características del Conjunto de Datos:\n",
    "\n",
    "- **Imágenes:** Cada imagen representa un dígito del 0 al 9.\n",
    "- **Tamaño:** Cada imagen tiene dimensiones de 28x28 píxeles.\n",
    "- **Etiquetas:** Cada imagen está etiquetada con el dígito que representa.\n",
    "\n",
    "El objetivo típico al trabajar con MNIST es entrenar un modelo de aprendizaje automático para reconocer y clasificar correctamente los dígitos escritos a mano basándose únicamente en las imágenes de entrada.\n",
    "\n",
    "![MNIST Dataset](img/MnistExamplesModified.png)\n",
    "\n",
    "## Importación del dataset MNIST y las herramientas necesarias para trabajar con él\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 07:00:17.540314: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-25 07:00:17.540790: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-25 07:00:17.544715: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-09-25 07:00:17.555318: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-25 07:00:17.576333: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-25 07:00:17.582624: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-25 07:00:17.601890: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-25 07:00:18.692875: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras.utils as np_utils\n",
    "from tensorflow.keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capas Layer and Dense\n",
    "### Clase `Layer`\n",
    "La clase Layer sirve como una clase base para todas las capas de una red neuronal. Contiene dos métodos esenciales, forward y backward, que deben ser implementados en las clases derivadas. Estos métodos representan el pase hacia adelante y el pase hacia atrás de la red.\n",
    "\n",
    "Pase hacia adelante (Forward Pass):\n",
    "Recibe una entrada de la capa anterior y calcula la salida para ser enviada a la siguiente capa.\n",
    "Pase hacia atrás (Backward Pass):\n",
    "Recibe el gradiente de la salida (de la capa siguiente) y actualiza los pesos u otros parámetros en base a la tasa de aprendizaje. También calcula el gradiente para la entrada, que se propagará hacia atrás.\n",
    "Atributos\n",
    "input: Los datos de entrada para la capa, almacenados durante el pase hacia adelante.\n",
    "output: La salida de la capa después del pase hacia adelante.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "\n",
    "La clase `Layer` es una clase base que define los métodos esenciales de una capa en una red neuronal. Incluye los métodos `forward` y `backward`, que son fundamentales para el entrenamiento de la red. Estos métodos deben ser implementados en las subclases derivadas de `Layer`.\n",
    "\n",
    "### Métodos\n",
    "\n",
    "*   **`forward(input)`** : Este método define el pase hacia adelante en la red. Toma como entrada `input` y devuelve la salida correspondiente de la capa. En la implementación base no realiza ninguna operación, ya que se espera que las subclases lo definan.\n",
    "    \n",
    "*   **`backward(output_gradient, learning_rate)`** : Este método define la retropropagación de la red. Recibe como parámetros el gradiente de la salida (`output_gradient`) y la tasa de aprendizaje (`learning_rate`). Actualiza los parámetros de la capa en base a estos valores. Similar al pase hacia adelante, su funcionalidad debe ser implementada en las subclases.\n",
    "    \n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Dense` (Capa Totalmente Conectada)\n",
    "-----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, 1)\n",
    "\n",
    "    def forward(self, input\n",
    "):\n",
    "        self.input = input\n",
    "        return np.dot(self.weights, self.input) + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = np.dot(output_gradient, self.input.T)\n",
    "        input_gradient = np.dot(self.weights.T, output_gradient)\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "\n",
    "La clase `Dense` implementa una capa totalmente conectada, donde cada neurona de la capa está conectada a todas las neuronas de la capa anterior. Se utiliza tanto para aprender características complejas como para realizar predicciones.\n",
    "\n",
    "### Atributos\n",
    "\n",
    "*   **`weights`** : Matriz de pesos con dimensiones `(output_size, input_size)` inicializada aleatoriamente. Estos pesos determinan la influencia de cada neurona de la capa anterior sobre las neuronas de la capa actual.\n",
    "    \n",
    "*   **`bias`** : Vector de sesgos con dimensiones `(output_size, 1)` también inicializado aleatoriamente. El sesgo se suma a la salida de la multiplicación de los pesos y la entrada.\n",
    "    \n",
    "\n",
    "### Métodos\n",
    "\n",
    "*   **`forward(input)`**: Realiza el pase hacia adelante multiplicando la entrada por los pesos y sumando el sesgo. La ecuación que describe esta operación es:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\text{salida} = W \\cdot X + b\n",
    "\\end{aligned}    \n",
    "$$\n",
    "\n",
    "Donde:\n",
    "\n",
    "*   $W$ es la matriz de pesos.\n",
    "*   $X$ es el vector de entrada.\n",
    "*   $b$ es el vector de sesgos.\n",
    "\n",
    "*   **`backward(output_gradient, learning_rate)`**: Realiza la retropropagación, calculando el gradiente de los pesos y el sesgo a partir del gradiente de la salida. Luego, actualiza los pesos y el sesgo con la tasa de aprendizaje.\n",
    "\n",
    "    Los gradientes se calculan de la siguiente manera:\n",
    "    \n",
    "    *   **Gradiente de los pesos**:\n",
    "\n",
    "    \n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial W} = \\text{gradiente\\_salida} \\cdot X^T\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "    *   **Gradiente de la entrada**:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\frac{\\partial L}{\\partial X} = W^T \\cdot \\text{gradiente\\_salida}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finalmente, los pesos y sesgos se actualizan con:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    W = W - \\text{tasa\\_aprendizaje} \\cdot \\frac{\\partial L}{\\partial W}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    b = b - \\text{tasa\\_aprendizaje} \\cdot \\frac{\\partial L}{\\partial b}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase `Activation`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        return np.multiply(output_gradient, self.activation_prime(self.input))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "\n",
    "La clase `Activation` representa una capa de activación en una red neuronal. Utiliza una función de activación dada y su derivada para aplicar transformaciones no lineales a la entrada durante el pase hacia adelante y hacia atrás.\n",
    "\n",
    "### Métodos\n",
    "\n",
    "*   **`forward(input)`** : Realiza el pase hacia adelante aplicando la función de activación a la entrada y guarda la entrada para su uso posterior en la retropropagación.\n",
    "    \n",
    "*   **`backward(output_gradient, learning_rate)`** : Realiza el pase hacia atrás multiplicando el gradiente de salida por la derivada de la función de activación evaluada en la entrada guardada. Este método ajusta la retropropagación de acuerdo con la transformación no lineal aplicada en el pase hacia adelante.\n",
    "    \n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Tanh`\n",
    "------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "\n",
    "La clase `Tanh` implementa la función de activación tangente hiperbólica y su derivada. Hereda de `Activation`, especificando la función `tanh` y su derivada `tanh_prime` como los métodos de activación y su derivada respectivamente.\n",
    "\n",
    "### Métodos\n",
    "\n",
    "No se agregan métodos adicionales más allá de los heredados de `Activation`.\n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Sigmoid`\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "\n",
    "        super().__init__(sigmoid, sigmoid_prime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "\n",
    "La clase `Sigmoid` implementa la función de activación sigmoide y su derivada. Al igual que `Tanh`, hereda de `Activation`, especificando la función `sigmoid` y su derivada `sigmoid_prime` como los métodos de activación y su derivada respectivamente.\n",
    "\n",
    "### Métodos\n",
    "\n",
    "No se agregan métodos adicionales más allá de los heredados de `Activation`.\n",
    "\n",
    "* * *\n",
    "\n",
    "Clase `Softmax`\n",
    "---------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Layer):\n",
    "    def forward(self, input\n",
    "):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descripción\n",
    "\n",
    "La clase `Softmax` implementa la función de activación softmax, comúnmente utilizada en la capa de salida de una red neuronal para problemas de clasificación multiclase. Calcula las probabilidades normalizadas de clases diferentes y sus gradientes durante el pase hacia adelante y hacia atrás, respectivamente.\n",
    "\n",
    "### Métodos\n",
    "\n",
    "*   **`forward(input)`** : Realiza el pase hacia adelante aplicando la función softmax a la entrada. Calcula exponenciales de la entrada, normaliza para obtener probabilidades y guarda el resultado en `self.output`.\n",
    "    \n",
    "*   **`backward(output_gradient, learning_rate)`** : Realiza el pase hacia atrás aplicando la derivada de softmax a `output_gradient`. Utiliza una forma optimizada de calcular el gradiente en comparación con la versión original, mejorando la eficiencia computacional durante la retropropagación.\n",
    "\n",
    "\n",
    "## Funciones de Pérdida y Derivadas\n",
    "\n",
    "### Función de Error Cuadrático Medio (MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula el error cuadrático medio entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - float: Error cuadrático medio.\n",
    "    \"\"\"\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula la derivada del error cuadrático medio respecto a las predicciones.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Gradiente del error cuadrático medio.\n",
    "    \"\"\"\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Entropía Cruzada Binaria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula la entropía cruzada binaria entre las predicciones y los valores verdaderos.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - float: Entropía cruzada binaria.\n",
    "    \"\"\"\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calcula la derivada de la entropía cruzada binaria respecto a las predicciones.\n",
    "\n",
    "    Args:\n",
    "    - y_true (numpy array): Valores verdaderos.\n",
    "    - y_pred (numpy array): Predicciones del modelo.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Gradiente de la entropía cruzada binaria.\n",
    "    \"\"\"\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Predicción y Entrenamiento de Redes Neuronales\n",
    "### Función de Predicción (`predict`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(network, input):\n",
    "    \"\"\"\n",
    "    Realiza una predicción utilizando una red neuronal dada.\n",
    "\n",
    "    Args:\n",
    "    - network (list): Lista de capas de la red neuronal.\n",
    "    - input (numpy array): Entrada para la predicción.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Salida de la red neuronal después de aplicar todas las capas.\n",
    "    \"\"\"\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Entrenamiento (`train`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(network, loss, loss_prime, x_train, y_train, epochs=1000, learning_rate=0.01, verbose=True):\n",
    "    \"\"\"\n",
    "    Entrena una red neuronal utilizando el algoritmo de retropropagación.\n",
    "\n",
    "    Args:\n",
    "    - network (list): Lista de capas de la red neuronal.\n",
    "    - loss (function): Función de pérdida para evaluar el error.\n",
    "    - loss_prime (function): Derivada de la función de pérdida para retropropagar el error.\n",
    "    - x_train (numpy array): Datos de entrada de entrenamiento.\n",
    "    - y_train (numpy array): Valores verdaderos correspondientes a los datos de entrada.\n",
    "    - epochs (int): Número de épocas o iteraciones de entrenamiento (default: 1000).\n",
    "    - learning_rate (float): Tasa de aprendizaje para actualizar los pesos durante el entrenamiento (default: 0.01).\n",
    "    - verbose (bool): Flag para imprimir el progreso del entrenamiento (default: True).\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "    \"\"\"\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = predict(network, x)\n",
    "\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "\n",
    "        error /= len(x_train)\n",
    "        if verbose:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Función de Preprocesamiento de Datos (`preprocess_data`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(x, y, limit):\n",
    "    \"\"\"\n",
    "    Preprocesa los datos de MNIST, transformando las imágenes y las etiquetas para su uso en la red neuronal.\n",
    "\n",
    "    Args:\n",
    "    - x (numpy array): Datos de entrada (imágenes).\n",
    "    - y (numpy array): Etiquetas correspondientes (números del 0 al 9).\n",
    "    - limit (int): Límite para el número de datos a preprocesar.\n",
    "\n",
    "    Returns:\n",
    "    - numpy array: Datos de entrada preprocesados.\n",
    "    - numpy array: Etiquetas preprocesadas y codificadas.\n",
    "    \"\"\"\n",
    "    # Reorganiza y normaliza los datos de entrada\n",
    "    x = x.reshape(x.shape[0], 28 * 28, 1)\n",
    "    x = x.astype(\"float32\") / 255\n",
    "    # Codifica la salida (números del 0 al 9) en un vector de tamaño 10\n",
    "    y = np_utils.to_categorical(y)\n",
    "    y = y.reshape(y.shape[0], 10, 1)\n",
    "    return x[:limit], y[:limit]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga y Preprocesamiento de Datos MNIST\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, y_train = preprocess_data(x_train, y_train, 1000)\n",
    "x_test, y_test = preprocess_data(x_test, y_test, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición de la Arquitectura de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = [\n",
    "    Dense(28 * 28, 40),\n",
    "    Tanh(),\n",
    "    Dense(40, 10),\n",
    "    Tanh()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento de la Red Neuronal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/100, error=0.8727098636309831\n",
      "2/100, error=0.8005264391101434\n",
      "3/100, error=0.7560818280770303\n",
      "4/100, error=0.6925861391497522\n",
      "5/100, error=0.5908179474134135\n",
      "6/100, error=0.43961604290773637\n",
      "7/100, error=0.26194002219622825\n",
      "8/100, error=0.1738515989570927\n",
      "9/100, error=0.14476697212888012\n",
      "10/100, error=0.1332438957178444\n",
      "11/100, error=0.12730190206612657\n",
      "12/100, error=0.12260658720032985\n",
      "13/100, error=0.11869035231345593\n",
      "14/100, error=0.11579003262922616\n",
      "15/100, error=0.11369414201779399\n",
      "16/100, error=0.1116697405507871\n",
      "17/100, error=0.10985325118990776\n",
      "18/100, error=0.10831724712648368\n",
      "19/100, error=0.10678412216485142\n",
      "20/100, error=0.10530855873483609\n",
      "21/100, error=0.10324705450081957\n",
      "22/100, error=0.10150586079577471\n",
      "23/100, error=0.09972885028166886\n",
      "24/100, error=0.09815220083130037\n",
      "25/100, error=0.09732193067348896\n",
      "26/100, error=0.09599288066333622\n",
      "27/100, error=0.09469321959716677\n",
      "28/100, error=0.09371754537997211\n",
      "29/100, error=0.09248000910317403\n",
      "30/100, error=0.09151283978733823\n",
      "31/100, error=0.09061713627686903\n",
      "32/100, error=0.08977133961612252\n",
      "33/100, error=0.08887872485160775\n",
      "34/100, error=0.08808315551447601\n",
      "35/100, error=0.08737590228643695\n",
      "36/100, error=0.0867703107040453\n",
      "37/100, error=0.08621801464950492\n",
      "38/100, error=0.08552185846674243\n",
      "39/100, error=0.08480905465283742\n",
      "40/100, error=0.0840643254936738\n",
      "41/100, error=0.0836957801459345\n",
      "42/100, error=0.08296125133524188\n",
      "43/100, error=0.08250204997339178\n",
      "44/100, error=0.08196749478602071\n",
      "45/100, error=0.08130255157890724\n",
      "46/100, error=0.08087233655240691\n",
      "47/100, error=0.08027273938971762\n",
      "48/100, error=0.07951030897200749\n",
      "49/100, error=0.07895267673572902\n",
      "50/100, error=0.07849533908100602\n",
      "51/100, error=0.07782248170364905\n",
      "52/100, error=0.07731430359139538\n",
      "53/100, error=0.07687977122588233\n",
      "54/100, error=0.07629637580687904\n",
      "55/100, error=0.07603039928937748\n",
      "56/100, error=0.07550779769094881\n",
      "57/100, error=0.07502359813051183\n",
      "58/100, error=0.0746086610140651\n",
      "59/100, error=0.0742512178649907\n",
      "60/100, error=0.07369672925976506\n",
      "61/100, error=0.07334658705718314\n",
      "62/100, error=0.07309618774742933\n",
      "63/100, error=0.07264913371506065\n",
      "64/100, error=0.07231292578980322\n",
      "65/100, error=0.072141063988483\n",
      "66/100, error=0.07167180380827479\n",
      "67/100, error=0.07125737945818496\n",
      "68/100, error=0.07110336869291416\n",
      "69/100, error=0.07051853980133753\n",
      "70/100, error=0.07015322578370915\n",
      "71/100, error=0.06987203906693279\n",
      "72/100, error=0.06952700176491002\n",
      "73/100, error=0.0692894637564653\n",
      "74/100, error=0.06869648325062735\n",
      "75/100, error=0.06826940073888742\n",
      "76/100, error=0.06770466994744617\n",
      "77/100, error=0.06741746736119415\n",
      "78/100, error=0.06702173766201777\n",
      "79/100, error=0.06673240922326762\n",
      "80/100, error=0.06638120203240204\n",
      "81/100, error=0.0661659867337441\n",
      "82/100, error=0.06585795033956811\n",
      "83/100, error=0.06562210165025661\n",
      "84/100, error=0.06532063160993853\n",
      "85/100, error=0.06503629295591996\n",
      "86/100, error=0.06477424187267533\n",
      "87/100, error=0.06455198452047989\n",
      "88/100, error=0.06429798082690026\n",
      "89/100, error=0.06402742337296816\n",
      "90/100, error=0.06373884947549117\n",
      "91/100, error=0.0634880274088557\n",
      "92/100, error=0.06318673361116552\n",
      "93/100, error=0.06289059338488802\n",
      "94/100, error=0.06255712081737637\n",
      "95/100, error=0.062378074177676684\n",
      "96/100, error=0.061921576033813604\n",
      "97/100, error=0.06168182434955905\n",
      "98/100, error=0.06134467327997239\n",
      "99/100, error=0.06111810674976954\n",
      "100/100, error=0.06085693808094799\n"
     ]
    }
   ],
   "source": [
    "train(network, mse, mse_prime, x_train, y_train, epochs=100, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba de la Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred: 7 \ttrue: 7\n",
      "pred: 5 \ttrue: 2\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 0 \ttrue: 4\n",
      "pred: 1 \ttrue: 1\n",
      "pred: 4 \ttrue: 4\n",
      "pred: 6 \ttrue: 9\n",
      "pred: 0 \ttrue: 5\n",
      "pred: 6 \ttrue: 9\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 0 \ttrue: 6\n",
      "pred: 6 \ttrue: 9\n",
      "pred: 0 \ttrue: 0\n",
      "pred: 6 \ttrue: 1\n",
      "pred: 7 \ttrue: 5\n",
      "pred: 2 \ttrue: 9\n",
      "pred: 7 \ttrue: 7\n",
      "pred: 6 \ttrue: 3\n",
      "pred: 4 \ttrue: 4\n"
     ]
    }
   ],
   "source": [
    "for x, y in zip(x_test, y_test):\n",
    "    output = predict(network, x)\n",
    "    print('pred:', np.argmax(output), '\\ttrue:', np.argmax(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusiones\n",
    "\n",
    "En este proyecto, hemos explorado el desarrollo y entrenamiento de una red neuronal convolucional (CNN) para la clasificación de dígitos utilizando el conjunto de datos MNIST. A continuación, se presentan las principales conclusiones y hallazgos obtenidos:\n",
    "\n",
    "### Logros\n",
    "\n",
    "- **Implementación Exitosa de la CNN:** Se logró implementar y entrenar una CNN utilizando la biblioteca Keras sobre TensorFlow. La red neuronal pudo aprender a reconocer los dígitos escritos a mano con una precisión significativa.\n",
    "\n",
    "- **Preprocesamiento Eficaz de Datos:** El preprocesamiento de las imágenes de MNIST, incluyendo la normalización y la codificación de las etiquetas, fue crucial para el éxito del modelo.\n",
    "\n",
    "- **Aprendizaje Automático de Representaciones:** La red neuronal pudo aprender representaciones significativas de las imágenes de dígitos, lo que permitió una clasificación precisa.\n",
    "\n",
    "### Desafíos y Lecciones Aprendidas\n",
    "\n",
    "- **Ajuste de Hiperparámetros:** La selección adecuada de hiperparámetros como la tasa de aprendizaje y el número de épocas de entrenamiento fue crucial y requirió experimentación y ajustes iterativos.\n",
    "\n",
    "- **Interpretación de Resultados:** La evaluación del modelo y la interpretación de las métricas de rendimiento fueron fundamentales para comprender su eficacia y posibles áreas de mejora.\n",
    "\n",
    "### Futuras Direcciones\n",
    "\n",
    "- **Mejoras en el Modelo:** Se podrían explorar arquitecturas más complejas de CNN, así como técnicas avanzadas como la regularización y el aumento de datos para mejorar aún más el rendimiento del modelo.\n",
    "\n",
    "- **Aplicaciones Prácticas:** Este proyecto puede extenderse para aplicaciones prácticas como sistemas de reconocimiento de caracteres en documentos escaneados o aplicaciones de OCR en tiempo real.\n",
    "\n",
    "En resumen, este proyecto no solo demuestra la aplicación efectiva de técnicas de aprendizaje automático para la clasificación de imágenes, sino que también destaca la importancia de la experimentación rigurosa y la evaluación exhaustiva en el desarrollo de modelos de aprendizaje automático."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
